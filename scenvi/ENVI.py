from functools import partial

import jax
import jax.numpy as jnp
import numpy as np
import optax
import pandas as pd
import scanpy as sc
import sklearn.neighbors
import tensorflow_probability.substrates.jax as jax_prob # type: ignore
from flax import linen as nn
from jax import jit, random
from tqdm import trange
from math import sqrt

import sys
sys.path.insert(1, '/home/chene5/ENVI_new_copy/scenvi')
from _dists import (
    KL,
    AOT_Distance,
    S2,
    log_nb_pdf,
    log_normal_pdf,
    log_pos_pdf,
    log_zinb_pdf,
)

from utils import CVAE, Metrics, TrainState, compute_covet, compute_niche, niche_cell_type, DefaultConfig


class ENVI:
    """
    Initializes the ENVI model & computes COVET for spatial data


    :param spatial_data: (anndata) spatial transcriptomics data, with an obsm indicating spatial location of spot/segmented cell
    :param sc_data: (anndata) complementary sinlge cell data
    :param spatial_key: (str) obsm key name with physical location of spots/cells (default 'spatial')
    :param batch_key: (str) obs key name of batch/sample of spatial data (default 'batch' if in spatial_data.obs, else -1)
    :param num_layers: (int) number of neural network for decoders and encoders (default 3)
    :param num_neurons: (int) number of neurons in each layer (default 1024)
    :param latent_dim: (int) size of ENVI latent dimention (size 512)
    :param k_nearest: (int) number of physical neighbours to describe niche (default 8)
    :param num_cov_genes: (int) number of HVGs to compute niche covariance with (default Ö¿64), if -1 uses all genes
    :param cov_genes: (list of str) manual genes to compute niche with (default [])
    :param num_HVG: (int) number of HVGs to keep for single cell data (default 2048)
    :param sc_genes: (list of str) manual genes to keep for sinlge cell data (default [])
    :param spatial_dist: (str) distribution used to describe spatial data (default pois, could be 'pois', 'nb', 'zinb' or 'norm')
    :param sc_dist: (str) distribution used to describe sinlge cell data (default nb, could be 'pois', 'nb', 'zinb' or 'norm')
    :param spatial_coeff: (float) coefficient for spatial expression loss in total ELBO (default 1.0)
    :param sc_coeff: (float) coefficient for sinlge cell expression loss in total ELBO (default 1.0)
    :param cov_coeff: (float) coefficient for spatial niche loss in total ELBO (default 1.0)
    :param kl_coeff: (float) coefficient for latent prior loss in total ELBO (default 1.0)
    :param log_input: (float) if larger than zero, a log is applied to ENVI input with pseudocount of log_input (default 0.1)
    :param stable_eps: (float) added value to log probabilty calculations to avoid NaNs during training (default 1e-6)

    :return: initialized ENVI model
    """

    def __init__(
        self,
        spatial_data,
        sc_data,
        spatial_key="spatial",
        batch_key="batch",
        num_layers=3,
        num_neurons=1024,
        latent_dim=512,
        k_nearest=8,
        #num_cov_genes=64,
        cov_genes=[],
        num_HVG=2048,
        sc_genes=[],
        spatial_dist="pois",
        sc_dist="nb",
        spatial_coeff=1,
        sc_coeff=1,
        #cov_coeff=1,
        niche_coeff=0.01,
        kl_coeff=0.3,
        log_input=0.1,
        stable_eps=1e-6,
    ):

        self.spatial_data = spatial_data[:, np.intersect1d(spatial_data.var_names, sc_data.var_names)]
        self.sc_data = sc_data
        if log_input > 0:
            self.spatial_data.layers["log"] = np.log(self.spatial_data.X + log_input)
            self.sc_data.layers["log"] = np.log(self.sc_data.X + log_input)

        if "highly_variable" not in sc_data.var.columns:
            sc_data.layers["log"] = np.log(sc_data.X + 1)
            sc.pp.highly_variable_genes(
                sc_data, layer="log", n_top_genes=min(num_HVG, sc_data.shape[-1])
            )

        sc_genes_keep = np.union1d(
            sc_data.var_names[sc_data.var.highly_variable], self.spatial_data.var_names
        )
        if len(sc_genes) > 0:
            sc_genes_keep = np.union1d(sc_genes_keep, sc_genes)

        if self.sc_data.raw is None:
            self.sc_data.raw = self.sc_data

        self.sc_data = self.sc_data[:, sc_genes_keep]

        self.overlap_genes = np.asarray(
            np.intersect1d(self.spatial_data.var_names, self.sc_data.var_names)
        )
        self.non_overlap_genes = np.asarray(
            list(set(self.sc_data.var_names) - set(self.spatial_data.var_names))
        )

        self.spatial_data = self.spatial_data[:, list(self.overlap_genes)]
        self.sc_data = self.sc_data[
            :, list(self.overlap_genes) + list(self.non_overlap_genes)
        ]

        if batch_key not in spatial_data.obs.columns:
            batch_key = -1

        self.k_nearest = k_nearest
        self.spatial_key = spatial_key
        self.batch_key = batch_key
        self.cov_genes = cov_genes
        #self.num_cov_genes = num_cov_genes
        self.overlap_num = self.overlap_genes.shape[0]
        # self.cov_gene_num = self.spatial_data.obsm["COVET_SQRT"].shape[-1]
        self.n_niche_genes = self.spatial_data.X.shape[-1]
        self.full_trans_gene_num = self.sc_data.shape[-1]


        # print("Computing Niche Covariance Matrices")

        # (
        #     self.spatial_data.obsm["COVET"],
        #     self.spatial_data.obsm["COVET_SQRT"],
        #     self.CovGenes,
        # ) = compute_covet(
        #     self.spatial_data,
        #     self.k_nearest,
        #     self.num_cov_genes,
        #     self.cov_genes,
        #     spatial_key=self.spatial_key,
        #     batch_key=self.batch_key,
        # )

        print("Computing Niche Matrices")
        (
            self.gene_mins,
            self.gene_maxs
        ) = compute_niche(
            self.spatial_data,
            self.n_niche_genes,
            self.k_nearest,
            spatial_key=self.spatial_key,
            batch_key=self.batch_key,
        )

        self.num_layers = num_layers
        self.num_neurons = num_neurons
        self.latent_dim = latent_dim

        self.spatial_dist = spatial_dist
        self.sc_dist = sc_dist

        self.dist_size_dict = {"pois": 1, "nb": 2, "zinb": 3, "norm": 1}

        self.exp_dec_size = (
            self.dist_size_dict[self.sc_dist] * self.sc_data.shape[-1]
            + (self.dist_size_dict[self.spatial_dist] - 1) * self.spatial_data.shape[-1]
        )

        self.spatial_coeff = spatial_coeff
        self.sc_coeff = sc_coeff
        # self.cov_coeff = cov_coeff
        self.niche_coeff = niche_coeff
        self.kl_coeff = kl_coeff

        if self.sc_dist == "norm" or self.spatial_dist == "norm":
            self.log_input = -1
        else:
            self.log_input = log_input

        self.eps = stable_eps

        print("Initializing CVAE")

        self.model = CVAE(
            n_layers=self.num_layers,
            n_neurons=self.num_neurons,
            n_latent=self.latent_dim,
            n_output_exp=self.exp_dec_size,
            # n_output_cov=int(self.cov_gene_num * (self.cov_gene_num + 1) / 2),
            k_nearest=self.k_nearest,
            n_niche_genes = self.n_niche_genes
        )

        print("Finished Initializing ENVI")

    def inp_log_fn(self, x):
        """
        :meta private:
        """

        if self.log_input > 0:
            return jnp.log(x + self.log_input)
        return x

    def mean_sc(self, sc_inp):
        """
        :meta private:
        """

        sc_inp = sc_inp[:, : self.dist_size_dict[self.sc_dist] * self.sc_data.shape[-1]]
        if self.sc_dist == "zinb":
            sc_r, sc_p, sc_d = jnp.split(sc_inp, 3, axis=-1)
            return nn.softplus(sc_r) * jnp.exp(sc_p) * (1 - nn.sigmoid(sc_d))
        if self.sc_dist == "nb":
            sc_r, sc_p = jnp.split(sc_inp, 2, axis=-1)
            return nn.softplus(sc_r) * jnp.exp(sc_p)
        if self.sc_dist == "pois":
            sc_l = sc_inp
            return sc_l
        if self.sc_dist == "norm":
            sc_l = sc_inp
            return sc_l

    def mean_spatial(self, spatial_inp):
        """
        :meta private:
        """

        if self.spatial_dist == "zinb" or self.spatial_dist == "nb":
            spatial_inp = jnp.concatenate(
                [
                    spatial_inp[:, : self.spatial_data.shape[-1]],
                    spatial_inp[
                        :,
                        -(self.dist_size_dict[self.spatial_dist] - 1)
                        * self.spatial_data.shape[-1] :,
                    ],
                ],
                axis=-1,
            )
        else:
            spatial_inp = spatial_inp[:, : self.spatial_data.shape[-1]]

        if self.spatial_dist == "zinb":
            spatial_r, spatial_p, spatial_d = jnp.split(spatial_inp, 3, axis=-1)
            return (
                nn.softplus(spatial_r)
                * jnp.exp(spatial_p)
                * (1 - nn.sigmoid(spatial_d))
            )
        if self.spatial_dist == "nb":
            spatial_r, spatial_p = jnp.split(spatial_inp, 2, axis=-1)
            return nn.softplus(spatial_r) * jnp.exp(spatial_p)
        if self.spatial_dist == "pois":
            spatial_l = spatial_inp
            return spatial_l
        if self.spatial_dist == "norm":
            spatial_l = spatial_inp
            return spatial_l

    def factor_sc(self, sc_inp, dec_exp):
        """
        :meta private:
        """

        sc_neurons = dec_exp[
            :, : self.dist_size_dict[self.sc_dist] * self.sc_data.shape[-1]
        ]

        if self.sc_dist == "zinb":
            sc_r, sc_p, sc_d = jnp.split(sc_neurons, 3, axis=-1)
            sc_like = jnp.mean(
                log_zinb_pdf(sc_inp, nn.softplus(sc_r) + self.eps, sc_p, sc_d)
            )
        if self.sc_dist == "nb":
            sc_r, sc_p = jnp.split(sc_neurons, 2, axis=-1)
            sc_like = jnp.mean(log_nb_pdf(sc_inp, nn.softplus(sc_r) + self.eps, sc_p))
        if self.sc_dist == "pois":
            sc_l = sc_neurons
            sc_like = jnp.mean(log_pos_pdf(sc_inp, nn.softplus(sc_l) + self.eps))
        if self.sc_dist == "norm":
            sc_l = sc_neurons
            sc_like = jnp.mean(log_normal_pdf(sc_inp, sc_l))
        return sc_like

    def factor_spatial(self, spatial_inp, dec_exp):
        """
        :meta private:
        """

        if self.spatial_dist == "zinb" or self.spatial_dist == "nb":
            spatial_neurons = jnp.concatenate(
                [
                    dec_exp[:, : self.spatial_data.shape[-1]],
                    dec_exp[
                        :,
                        -(self.dist_size_dict[self.spatial_dist] - 1)
                        * self.spatial_data.shape[-1] :,
                    ],
                ],
                axis=-1,
            )
        else:
            spatial_neurons = dec_exp[:, : self.spatial_data.shape[-1]]

        if self.spatial_dist == "zinb":
            spatial_r, spatial_p, spatial_d = jnp.split(spatial_neurons, 3, axis=-1)
            spatial_like = jnp.mean(
                log_zinb_pdf(
                    spatial_inp, nn.softplus(spatial_r) + self.eps, spatial_p, spatial_d
                )
            )
        if self.spatial_dist == "nb":
            spatial_r, spatial_p = jnp.split(spatial_neurons, 2, axis=-1)
            spatial_like = jnp.mean(
                log_nb_pdf(spatial_inp, nn.softplus(spatial_r) + self.eps, spatial_p)
            )
        if self.spatial_dist == "pois":
            spatial_l = spatial_neurons
            spatial_like = jnp.mean(
                log_pos_pdf(spatial_inp, nn.softplus(spatial_l) + self.eps)
            )
        if self.spatial_dist == "norm":
            spatial_l = spatial_neurons
            spatial_like = jnp.mean(log_normal_pdf(spatial_inp, spatial_l))
        return spatial_like

    def grammian_cov(self, dec_cov):
        """
        :meta private:
        """

        dec_cov = jax_prob.math.fill_triangular(dec_cov)
        return jnp.matmul(dec_cov, dec_cov.transpose([0, 2, 1]))

    def batched_S2(self, x, y, epsilon, lse_mode):
        """
        Helper function to run S2 on x and y (batches of 2D matrices)
        :return: mean_OT_dists, OT_dists 
                OT_dists is a (batchsize,) array
        """

        OT_dists = jax.vmap(lambda x, y: S2(x, y, epsilon, lse_mode), in_axes=[0,0])(x, y)
        return jnp.mean(OT_dists), OT_dists

    def create_train_state(self, key=random.key(0), init_lr=0.0001, decay_steps=4000):
        """
        :meta private:
        """

        key, subkey1, subkey2 = random.split(key, num=3)
        params = self.model.init(
            rngs={"params": subkey1},
            x=self.inp_log_fn(self.spatial_data.X[0:1]),
            mode="spatial",
            key=subkey2,
        )["params"]

        lr_sched = optax.exponential_decay(init_lr, decay_steps, 0.5, staircase=True)
        tx = optax.adam(lr_sched)  #

        return TrainState.create(
            apply_fn=self.model.apply, params=params, tx=tx, metrics=Metrics.empty()
        )

    @partial(jit, static_argnums=(0,))
    def train_step(self, state, spatial_inp, spatial_niche, sc_inp, key=random.key(0)):
        """
        :meta private:
        """

        key, subkey1, subkey2 = random.split(key, num=3)

        def loss_fn(params): # redefine loss function
            # spatial_enc_mu, spatial_enc_logstd, spatial_dec_exp, spatial_dec_cov = (
            spatial_enc_mu, spatial_enc_logstd, spatial_dec_exp, spatial_dec_niche = (
                state.apply_fn(
                    {"params": params},
                    x=self.inp_log_fn(spatial_inp),
                    mode="spatial",
                    key=subkey1,
                )
            )
            sc_enc_mu, sc_enc_logstd, sc_dec_exp = state.apply_fn(
                {"params": params},
                x=self.inp_log_fn(sc_inp[:, : spatial_inp.shape[-1]]),
                mode="sc",
                key=subkey2,
            )

            spatial_exp_like = self.factor_spatial(spatial_inp, spatial_dec_exp)
            sc_exp_like = self.factor_sc(sc_inp, sc_dec_exp)

            # spatial_cov_like = jnp.mean(
            #     AOT_Distance(spatial_COVET, self.grammian_cov(spatial_dec_cov))
            # )

            spatial_niche_cost = self.batched_S2(spatial_niche, 
                                                 spatial_dec_niche, 
                                                 epsilon=1e-2, 
                                                 lse_mode=True)[0]

            kl_div = jnp.mean(KL(spatial_enc_mu, spatial_enc_logstd)) + jnp.mean(
                KL(sc_enc_mu, sc_enc_logstd)
            )

            loss = (
                - self.spatial_coeff * spatial_exp_like
                - self.sc_coeff * sc_exp_like
                # - self.cov_coeff * spatial_cov_like
                + self.niche_coeff * spatial_niche_cost
                + self.kl_coeff * kl_div
            )

            return (
                loss,
                # [sc_exp_like, spatial_exp_like, spatial_cov_like, kl_div * 0.5],
                [sc_exp_like, spatial_exp_like, spatial_niche_cost, kl_div * 0.5],
            )

        grad_fn = jax.value_and_grad(loss_fn, has_aux=True)
        loss, grads = grad_fn(state.params)
        state = state.apply_gradients(grads=grads)
        return (state, loss)

    def train(
        self,
        training_steps=16000,
        batch_size=128,
        verbose=16,
        init_lr=0.0001,
        decay_steps=4000,
        key=random.key(0),
    ):
        """
        Set up optimization parameters and train the ENVI moodel


        :param training_steps: (int) number of gradient descent steps to train ENVI (default 16000)
        :param batch_size: (int) size of spatial and single-cell profiles sampled for each training step  (default 128)
        :param verbose: (int) amount of steps between each loss print statement (default 16)
        :param init_lr: (float) initial learning rate for ADAM optimizer with exponential decay (default 1e-4)
        :param decay_steps: (int) number of steps before each learning rate decay (default 4000)
        :param key: (jax.random.key) random seed (default jax.random.key(0))

        :return: nothing
        """

        batch_size = min(
            self.sc_data.shape[0], min(self.spatial_data.shape[0], batch_size)
        )

        key, subkey = random.split(key)
        state = self.create_train_state(
            subkey, init_lr=init_lr, decay_steps=decay_steps
        )
        self.state = state # save state for later
        self.params = state.params

        tq = trange(training_steps, leave=True, desc="")
        sc_loss_mean, spatial_loss_mean, niche_loss_mean, kl_loss_mean, count = (
            0,
            0,
            0,
            0,
            0,
        )

        sc_X = self.sc_data.X
        spatial_X = self.spatial_data.X
        # spatial_COVET = self.spatial_data.obsm["COVET_SQRT"]
        spatial_niche = self.spatial_data.obsm["scaled_niche"]

        for training_step in tq:
            key, subkey1, subkey2 = random.split(key, num=3)

            batch_spatial_ind = random.choice(
                key=subkey1,
                a=self.spatial_data.shape[0],
                shape=[batch_size],
                replace=False,
            )
            batch_sc_ind = random.choice(
                key=subkey2, a=self.sc_data.shape[0], shape=[batch_size], replace=False
            )

            # batch_spatial_exp, batch_spatial_cov = (
            #     spatial_X[batch_spatial_ind],
            #     spatial_COVET[batch_spatial_ind],
            # )
            batch_spatial_exp, batch_spatial_niche = (
                spatial_X[batch_spatial_ind],
                spatial_niche[batch_spatial_ind],
            )
            batch_sc_exp = sc_X[batch_sc_ind]

            key, subkey = random.split(key)

            state, loss = self.train_step(
                # state, batch_spatial_exp, batch_spatial_cov, batch_sc_exp, key=subkey
                state, batch_spatial_exp, batch_spatial_niche, batch_sc_exp, key=subkey
            )

            self.params = state.params

            sc_loss_mean, spatial_loss_mean, niche_loss_mean, kl_loss_mean, count = (
                sc_loss_mean + loss[1][0],
                spatial_loss_mean + loss[1][1],
                niche_loss_mean + loss[1][2],
                kl_loss_mean + loss[1][3],
                count + 1,
            )

            if training_step % verbose == 0:
                print_statement = ""
                for metric, value in zip(
                    ["spatial", "sc", "niche", "kl"],
                    [spatial_loss_mean, sc_loss_mean, niche_loss_mean, kl_loss_mean],
                ):
                    print_statement = (
                        print_statement
                        + " "
                        + metric
                        + ": {:.3e}".format(value / count)
                    )

                sc_loss_mean, spatial_loss_mean, niche_loss_mean, kl_loss_mean, count = (
                    0,
                    0,
                    0,
                    0,
                    0,
                )
                tq.set_description(print_statement)
                tq.refresh()  # to show

        self.latent_rep()

    def start_train(
            self,
            init_lr=0.0001,
            decay_steps=4000,
            key=random.key(0)
    ):
        key, subkey = random.split(key)
        state = self.create_train_state(
            subkey, init_lr=init_lr, decay_steps=decay_steps
        )
        self.state = state # save state for later
        self.params = state.params

    def continue_train(  
        self,
        training_steps=16000,
        batch_size=128,
        verbose=16,
        key=random.key(0),
    ):
        '''
        continue training with pre-existing train state
        '''

        batch_size = min(
            self.sc_data.shape[0], min(self.spatial_data.shape[0], batch_size)
        )

        state = self.state 
        self.params = state.params

        tq = trange(training_steps, leave=True, desc="")
        sc_loss_mean, spatial_loss_mean, niche_loss_mean, kl_loss_mean, count = (
            0,
            0,
            0,
            0,
            0,
        )

        sc_X = self.sc_data.X
        spatial_X = self.spatial_data.X
        # spatial_COVET = self.spatial_data.obsm["COVET_SQRT"]
        spatial_niche = self.spatial_data.obsm["scaled_niche"]

        for training_step in tq:
            key, subkey1, subkey2 = random.split(key, num=3)

            batch_spatial_ind = random.choice(
                key=subkey1,
                a=self.spatial_data.shape[0],
                shape=[batch_size],
                replace=False,
            )
            batch_sc_ind = random.choice(
                key=subkey2, a=self.sc_data.shape[0], shape=[batch_size], replace=False
            )

            # batch_spatial_exp, batch_spatial_cov = (
            #     spatial_X[batch_spatial_ind],
            #     spatial_COVET[batch_spatial_ind],
            # )
            batch_spatial_exp, batch_spatial_niche = (
                spatial_X[batch_spatial_ind],
                spatial_niche[batch_spatial_ind],
            )
            batch_sc_exp = sc_X[batch_sc_ind]

            key, subkey = random.split(key)

            state, loss = self.train_step(
                # state, batch_spatial_exp, batch_spatial_cov, batch_sc_exp, key=subkey
                state, batch_spatial_exp, batch_spatial_niche, batch_sc_exp, key=subkey
            )

            self.params = state.params

            sc_loss_mean, spatial_loss_mean, niche_loss_mean, kl_loss_mean, count = (
                sc_loss_mean + loss[1][0],
                spatial_loss_mean + loss[1][1],
                niche_loss_mean + loss[1][2],
                kl_loss_mean + loss[1][3],
                count + 1,
            )

            if training_step % verbose == 0:
                print_statement = ""
                for metric, value in zip(
                    ["spatial", "sc", "niche", "kl"],
                    [spatial_loss_mean, sc_loss_mean, niche_loss_mean, kl_loss_mean],
                ):
                    print_statement = (
                        print_statement
                        + " "
                        + metric
                        + ": {:.3e}".format(value / count)
                    )

                sc_loss_mean, spatial_loss_mean, niche_loss_mean, kl_loss_mean, count = (
                    0,
                    0,
                    0,
                    0,
                    0,
                )
                tq.set_description(print_statement)
                tq.refresh()  # to show

        self.latent_rep()

    #@partial(jit, static_argnums=(0,))
    def model_encoder(self, x):
        """
        :meta private:
        """

        return self.model.bind({"params": self.params}).encoder(x)

    #@partial(jit, static_argnums=(0,))
    def model_decoder_exp(self, x):
        """
        :meta private:
        """

        return self.model.bind({"params": self.params}).decoder_exp(x)

    # @partial(jit, static_argnums=(0,))
    # def model_decoder_cov(self, x):
    #     """
    #     :meta private:
    #     """
    #     return self.model.bind({"params": self.params}).decoder_cov(x)
    
    #@partial(jit, static_argnums=(0,))
    def model_decoder_niche(self, x):
        """
        :meta private:
        """
        return self.model.bind({"params": self.params}).decoder_niche(x)
        

    def encode(self, x, mode="spatial", max_batch=256):
        """
        :meta private:
        """

        conf_const = 0 if mode == "spatial" else 1
        conf_neurons = jax.nn.one_hot(
            conf_const * jnp.ones(x.shape[0], dtype=jnp.int8), 2, dtype=jnp.float32
        )

        x_conf = jnp.concatenate([self.inp_log_fn(x), conf_neurons], axis=-1)

        if x_conf.shape[0] < max_batch:
            enc = jnp.split(self.model_encoder(x_conf), 2, axis=-1)[0]
        else:  # For when the GPU can't pass all point-clouds at once
            num_split = int(x_conf.shape[0] / max_batch) + 1
            x_conf_split = np.array_split(x_conf, num_split)
            enc = np.concatenate(
                [
                    jnp.split(self.model_encoder(x_conf_split[split_ind]), 2, axis=-1)[
                        0
                    ]
                    for split_ind in range(num_split)
                ],
                axis=0,
            )
        return enc

    def decode_exp(self, x, mode="spatial", max_batch=256):
        """
        :meta private:
        """

        conf_const = 0 if mode == "spatial" else 1
        conf_neurons = jax.nn.one_hot(
            conf_const * jnp.ones(x.shape[0], dtype=jnp.int8), 2, dtype=jnp.float32
        )

        x_conf = jnp.concatenate([x, conf_neurons], axis=-1)

        if mode == "spatial":
            if x_conf.shape[0] < max_batch:
                dec = self.mean_spatial(self.model_decoder_exp(x_conf))
            else:  # For when the GPU can't pass all point-clouds at once
                num_split = int(x_conf.shape[0] / max_batch) + 1
                x_conf_split = np.array_split(x_conf, num_split)
                dec = np.concatenate(
                    [
                        self.mean_spatial(
                            self.model_decoder_exp(x_conf_split[split_ind])
                        )
                        for split_ind in range(num_split)
                    ],
                    axis=0,
                )
        else:
            if x_conf.shape[0] < max_batch:
                dec = self.mean_sc(
                    self.model.bind({"params": self.params}).decoder_exp(x_conf)
                )
            else:  # For when the GPU can't pass all point-clouds at once
                num_split = int(x_conf.shape[0] / max_batch) + 1
                x_conf_split = np.array_split(x_conf, num_split)
                dec = np.concatenate(
                    [
                        self.mean_sc(self.model_decoder_exp(x_conf_split[split_ind]))
                        for split_ind in range(num_split)
                    ],
                    axis=0,
                )
        return dec

    # def decode_cov(self, x, max_batch=256):
    #     """
    #     :meta private:
    #     """

    #     conf_const = 0
    #     conf_neurons = jax.nn.one_hot(
    #         conf_const * jnp.ones(x.shape[0], dtype=jnp.int8), 2, dtype=jnp.float32
    #     )

    #     x_conf = jnp.concatenate([x, conf_neurons], axis=-1)

    #     if x_conf.shape[0] < max_batch:
    #         dec = self.grammian_cov(self.model_decoder_cov(x_conf))
    #     else:  # For when the GPU can't pass all point-clouds at once
    #         num_split = int(x_conf.shape[0] / max_batch) + 1
    #         x_conf_split = np.array_split(x_conf, num_split)
    #         dec = np.concatenate(
    #             [
    #                 self.grammian_cov(self.model_decoder_cov(x_conf_split[split_ind]))
    #                 for split_ind in range(num_split)
    #             ],
    #             axis=0,
    #         )
    #     return dec
    
    def decode_niche(self, x, max_batch=256):
        """
        :meta private:
        """

        conf_const = 0
        conf_neurons = jax.nn.one_hot(
            conf_const * jnp.ones(x.shape[0], dtype=jnp.int8), 2, dtype=jnp.float32
        )

        x_conf = jnp.concatenate([x, conf_neurons], axis=-1)

        if x_conf.shape[0] < max_batch:
            dec = self.model_decoder_niche(x_conf)
        else:  # For when the GPU can't pass all point-clouds at once
            num_split = int(x_conf.shape[0] / max_batch) + 1
            x_conf_split = np.array_split(x_conf, num_split)
            dec = np.concatenate(
                [
                    self.model_decoder_niche(x_conf_split[split_ind])
                    for split_ind in range(num_split)
                ],
                axis=0,
            )
        
        return dec

    def latent_rep(self):
        """
        Compute latent embeddings for spatial and single cell data, automatically performed after training

        :return: nothing, adds 'envi_latent' self.spatial_data.obsm and self.spatial_data.obsm
        """

        self.spatial_data.obsm["envi_latent"] = self.encode(
            self.spatial_data.X, mode="spatial"
        )
        self.sc_data.obsm["envi_latent"] = self.encode(
            self.sc_data[:, self.spatial_data.var_names].X, mode="sc"
        )

    def latent_rep_x(self, x, mode):
        """
        Compute latent embeddings for spatial and single cell data, automatically performed after training

        :return: nothing, adds 'envi_latent' self.spatial_data.obsm and self.spatial_data.obsm
        """
        if mode=="spatial":
            input = x.X
        elif mode=="sc":
            input = x[:, self.spatial_data.var_names].X

        x.obsm["envi_latent"] = self.encode(
            input, mode=mode
        )

    def impute_genes(self):
        """
        Impute full transcriptome for spatial data

        :return: nothing, adds 'imputation' to self.spatial_data.obsm
        """
        # does not re-call latent_rep method. make sure latent is up-to-date

        self.spatial_data.obsm["imputation"] = pd.DataFrame(
            self.decode_exp(self.spatial_data.obsm["envi_latent"], mode="sc"),
            columns=self.sc_data.var_names,
            index=self.spatial_data.obs_names,
        )

        print(
            "Finished imputing missing gene for spatial data! See 'imputation' in obsm of ENVI.spatial_data"
        )

    # def infer_niche_covet(self):
    #     """
    #     Predict COVET representation for single-cell data

    #     :return: nothing, adds 'COVET_SQRT' and 'COVET' to self.sc_data.obsm
    #     """

    #     self.sc_data.obsm["COVET_SQRT"] = self.decode_cov(
    #         self.sc_data.obsm["envi_latent"]
    #     )
    #     self.sc_data.obsm["COVET"] = np.matmul(
    #         self.sc_data.obsm["COVET_SQRT"], self.sc_data.obsm["COVET_SQRT"]
    #     )


    def infer_niche_x(self, x, mode):
        """
        Predict niche representation for single-cell data

        :return: nothing, adds 'niche' to self.sc_data.obsm
        """

        self.latent_rep_x(x, mode=mode)

        x.obsm["inferred_scaled_niche"] = self.decode_niche(
            x.obsm["envi_latent"]
        )
        x.obsm["inferred_niche"] = (x.obsm["inferred_scaled_niche"] * sqrt(self.n_niche_genes) + 1) / 2 * (self.gene_maxs - self.gene_mins) + self.gene_mins
    

    def infer_niche_sc(self):
        """
        Predict niche representation for single-cell data

        :return: nothing, adds 'inferred_niche' to self.sc_data.obsm
        """
        self.infer_niche_x(self.sc_data, mode="sc")


    def infer_niche_st(self):
        """
        For validation

        :return: nothing, adds 'inferred_scaled_niche' and 'inferred_niche' to self.sc_data.obsm
        """
        self.infer_niche_x(self.spatial_data, mode="spatial")


    def infer_niche_celltype(self, cell_type_key="cell_type"):
        """
        Predict cell type abundence based one ENVI-inferred COVET representations

        :param cell_type_key: (string) key in spatial_data.obs where cell types are stored for environment composition (default 'cell_type')

        :return: nothing, adds 'niche_cell_type' to self.sc_data.obsm & self.spatial_data.obsm
        """

        self.spatial_data.obsm["cell_type_niche"] = niche_cell_type(
            self.spatial_data,
            self.k_nearest,
            spatial_key=self.spatial_key,
            cell_type_key=cell_type_key,
            batch_key=self.batch_key,
        )

        regression_model = sklearn.neighbors.KNeighborsRegressor(n_neighbors=5).fit(
            self.spatial_data.obsm["scaled_niche"].reshape(
                [self.spatial_data.shape[0], -1]
            ),
            self.spatial_data.obsm["cell_type_niche"],
        )

        sc_cell_type = regression_model.predict(
            self.sc_data.obsm["inferred_scaled_niche"].reshape([self.sc_data.shape[0], -1])
        )

        self.sc_data.obsm["cell_type_niche"] = pd.DataFrame(
            sc_cell_type,
            index=self.sc_data.obs_names,
            columns=self.spatial_data.obsm["cell_type_niche"].columns,
        )


